#!/usr/bin/env python3
"""
Script para reanudar el pipeline desde el paso 5
"""

from datetime import datetime
import json
import logging
from pathlib import Path
import sys

import pandas as pd

from dataset_orchestrator import DatasetConfig, DatasetOrchestrator


def resume_from_step5(dataset_dir: str):
    """Reanuda el pipeline desde el paso 5"""

    dataset_path = Path(dataset_dir)

    # Verificar que los archivos necesarios existen
    embeddings_file = dataset_path / "embeddings" / "embeddings_data.pkl"
    indices_dir = dataset_path / "indices"

    if not embeddings_file.exists():
        logging.error(f"âŒ Error: No se encontrÃ³ {embeddings_file}")
        return False

    if not indices_dir.exists():
        logging.error(f"âŒ Error: No se encontrÃ³ {indices_dir}")
        return False

    logging.info(f"âœ… Encontrados archivos necesarios en {dataset_dir}")

    # Cargar DataFrame con embeddings
    logging.info("ğŸ“Š Cargando DataFrame con embeddings...")
    df_with_embeddings = pd.read_pickle(embeddings_file)
    logging.info(f"âœ… Cargado DataFrame con {len(df_with_embeddings)} segmentos")

    # Cargar metadatos de Ã­ndices
    indices_metadata_file = indices_dir / "indices_metadata.json"
    if indices_metadata_file.exists():
        with open(indices_metadata_file, encoding='utf-8') as f:
            indices_info = json.load(f)
        logging.info("âœ… Metadatos de Ã­ndices cargados")
    else:
        # Crear metadatos bÃ¡sicos
        indices_info = {
            "indices_dir": str(indices_dir),
            "text_index": str(indices_dir / "text_index.faiss"),
            "audio_index": str(indices_dir / "audio_index.faiss"),
            "metadata": str(indices_metadata_file)
        }
        logging.warning("âš ï¸  Usando metadatos de Ã­ndices bÃ¡sicos")

    # Crear configuraciÃ³n bÃ¡sica para el orquestador
    config = DatasetConfig(
        input_dir="./data",  # No importa para este paso
        output_dir=str(dataset_path)
    )

    # Crear orquestador
    orchestrator = DatasetOrchestrator(config)

    # Simular estadÃ­sticas
    orchestrator.stats.total_files = len(df_with_embeddings['source_file'].unique())
    orchestrator.stats.total_segments = len(df_with_embeddings)
    orchestrator.stats.converted_files = orchestrator.stats.total_files
    orchestrator.stats.transcribed_files = orchestrator.stats.total_files
    orchestrator.stats.embedded_files = orchestrator.stats.total_files
    orchestrator.stats.start_time = datetime.now()

    try:
        logging.info("ğŸš€ Ejecutando paso 5: CreaciÃ³n de dataset final...")
        final_dataset = orchestrator.step5_create_final_dataset(df_with_embeddings, indices_info)

        logging.info("âœ… Â¡Dataset final creado exitosamente!")
        logging.info(f"ğŸ“ Dataset completo en: {final_dataset['dataset_dir']}")
        logging.info(f"ğŸ“‹ Manifiesto: {final_dataset['manifest']}")

        return True

    except Exception as e:
        logging.error(f"âŒ Error en paso 5: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) != 2:
        logging.error("Uso: python resume_pipeline.py <directorio_dataset>")
        logging.error("Ejemplo: python resume_pipeline.py ./dataset")
        sys.exit(1)

    dataset_dir = sys.argv[1]
    success = resume_from_step5(dataset_dir)

    if success:
        logging.info("\nğŸ‰ Pipeline reanudado exitosamente!")
        sys.exit(0)
    else:
        logging.error("\nâŒ Error al reanudar pipeline")
        sys.exit(1)
