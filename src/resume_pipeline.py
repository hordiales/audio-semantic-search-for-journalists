#!/usr/bin/env python3
"""
Script para reanudar el pipeline desde el paso 5
"""

import pandas as pd
from pathlib import Path
import json
from datetime import datetime
import sys

from dataset_orchestrator import DatasetOrchestrator, DatasetConfig

def resume_from_step5(dataset_dir: str):
    """Reanuda el pipeline desde el paso 5"""
    
    dataset_path = Path(dataset_dir)
    
    # Verificar que los archivos necesarios existen
    embeddings_file = dataset_path / "embeddings" / "embeddings_data.pkl"
    indices_dir = dataset_path / "indices"
    
    if not embeddings_file.exists():
        print(f"âŒ Error: No se encontrÃ³ {embeddings_file}")
        return False
    
    if not indices_dir.exists():
        print(f"âŒ Error: No se encontrÃ³ {indices_dir}")
        return False
    
    print(f"âœ… Encontrados archivos necesarios en {dataset_dir}")
    
    # Cargar DataFrame con embeddings
    print("ğŸ“Š Cargando DataFrame con embeddings...")
    df_with_embeddings = pd.read_pickle(embeddings_file)
    print(f"âœ… Cargado DataFrame con {len(df_with_embeddings)} segmentos")
    
    # Cargar metadatos de Ã­ndices
    indices_metadata_file = indices_dir / "indices_metadata.json"
    if indices_metadata_file.exists():
        with open(indices_metadata_file, 'r', encoding='utf-8') as f:
            indices_info = json.load(f)
        print("âœ… Metadatos de Ã­ndices cargados")
    else:
        # Crear metadatos bÃ¡sicos
        indices_info = {
            "indices_dir": str(indices_dir),
            "text_index": str(indices_dir / "text_index.faiss"),
            "audio_index": str(indices_dir / "audio_index.faiss"),
            "metadata": str(indices_metadata_file)
        }
        print("âš ï¸  Usando metadatos de Ã­ndices bÃ¡sicos")
    
    # Crear configuraciÃ³n bÃ¡sica para el orquestador
    config = DatasetConfig(
        input_dir="./data",  # No importa para este paso
        output_dir=str(dataset_path)
    )
    
    # Crear orquestador
    orchestrator = DatasetOrchestrator(config)
    
    # Simular estadÃ­sticas
    orchestrator.stats.total_files = len(df_with_embeddings['source_file'].unique())
    orchestrator.stats.total_segments = len(df_with_embeddings)
    orchestrator.stats.converted_files = orchestrator.stats.total_files
    orchestrator.stats.transcribed_files = orchestrator.stats.total_files
    orchestrator.stats.embedded_files = orchestrator.stats.total_files
    orchestrator.stats.start_time = datetime.now()
    
    try:
        print("ğŸš€ Ejecutando paso 5: CreaciÃ³n de dataset final...")
        final_dataset = orchestrator.step5_create_final_dataset(df_with_embeddings, indices_info)
        
        print("âœ… Â¡Dataset final creado exitosamente!")
        print(f"ğŸ“ Dataset completo en: {final_dataset['dataset_dir']}")
        print(f"ğŸ“‹ Manifiesto: {final_dataset['manifest']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en paso 5: {e}")
        return False

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Uso: python resume_pipeline.py <directorio_dataset>")
        print("Ejemplo: python resume_pipeline.py ./dataset")
        sys.exit(1)
    
    dataset_dir = sys.argv[1]
    success = resume_from_step5(dataset_dir)
    
    if success:
        print("\nğŸ‰ Pipeline reanudado exitosamente!")
        sys.exit(0)
    else:
        print("\nâŒ Error al reanudar pipeline")
        sys.exit(1)