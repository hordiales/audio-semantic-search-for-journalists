#!/usr/bin/env python3
"""
Test y comparaci√≥n de rendimiento entre bases de datos vectoriales:
FAISS, ChromaDB, Supabase y Memoria
"""

import sys
import os
import logging
import time
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any
import json

CURRENT_FILE = Path(__file__).resolve()
TESTS_ROOT = CURRENT_FILE
while TESTS_ROOT.name != "tests" and TESTS_ROOT.parent != TESTS_ROOT:
    TESTS_ROOT = TESTS_ROOT.parent
if str(TESTS_ROOT) not in sys.path:
    sys.path.insert(0, str(TESTS_ROOT))

from tests.common.path_utils import artifacts_dir, ensure_sys_path, SRC_ROOT

ensure_sys_path([SRC_ROOT])
OUTPUT_ROOT = artifacts_dir("vector_databases")

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_vector_databases():
    """Prueba y compara el rendimiento de las bases de datos vectoriales"""
    print("üß™ Test de Bases de Datos Vectoriales")
    print("=" * 80)

    try:
        from vector_database_interface import VectorDBType, VectorDocument, create_vector_database
        from vector_database_config import get_configurator, ConfigurationPreset

        # Configurar para pruebas
        configurator = get_configurator("test_vector_db_config.json")
        configurator.apply_preset(ConfigurationPreset.DEVELOPMENT)

        # Definir bases de datos a probar
        databases_to_test = [
            VectorDBType.MEMORY,
            VectorDBType.FAISS,
            # VectorDBType.CHROMADB,  # Descomentaremos si est√° disponible
            # VectorDBType.SUPABASE   # Requiere configuraci√≥n espec√≠fica
        ]

        # Generar datos de prueba
        test_data = generate_test_data(num_docs=1000, embedding_dim=256)
        test_queries = generate_test_queries(num_queries=50, embedding_dim=256)

        print(f"üìä Datos de prueba generados:")
        print(f"   üìÑ Documentos: {len(test_data)}")
        print(f"   üîç Consultas: {len(test_queries)}")

        # Resultados de pruebas
        results = {}

        # Probar cada base de datos
        for db_type in databases_to_test:
            print(f"\n{'='*60}")
            print(f"üß™ Probando: {db_type.value.upper()}")
            print(f"{'='*60}")

            try:
                result = test_single_database(db_type, configurator, test_data, test_queries)
                results[db_type.value] = result
                print(f"‚úÖ {db_type.value}: Prueba completada")

            except Exception as e:
                print(f"‚ùå {db_type.value}: Error - {e}")
                results[db_type.value] = {"error": str(e)}

        # Generar reporte de comparaci√≥n
        generate_comparison_report(results)

        return True

    except Exception as e:
        print(f"‚ùå Error en tests: {e}")
        import traceback
        traceback.print_exc()
        return False

def generate_test_data(num_docs: int, embedding_dim: int) -> List[VectorDocument]:
    """Genera datos de prueba sint√©ticos"""
    print(f"üìä Generando {num_docs} documentos de prueba...")

    np.random.seed(42)  # Para reproducibilidad
    documents = []

    categories = ['pol√≠tica', 'econom√≠a', 'tecnolog√≠a', 'salud', 'deportes']
    sources = ['radio', 'podcast', 'entrevista', 'debate', 'noticia']

    for i in range(num_docs):
        # Generar embedding con estructura sem√°ntica
        category_idx = i % len(categories)
        category = categories[category_idx]

        # Crear embedding con correlaci√≥n por categor√≠a
        base_embedding = np.random.randn(embedding_dim) * 0.1
        category_signal = np.zeros(embedding_dim)
        start_idx = category_idx * (embedding_dim // len(categories))
        end_idx = min(start_idx + (embedding_dim // len(categories)), embedding_dim)
        category_signal[start_idx:end_idx] = 1.0 + np.random.randn(end_idx - start_idx) * 0.2

        embedding = base_embedding + category_signal * 0.8
        embedding = embedding / np.linalg.norm(embedding)  # Normalizar

        # Crear documento
        doc = VectorDocument(
            id=f"doc_{i:04d}",
            embedding=embedding,
            text=f"Contenido sobre {category} n√∫mero {i}. Informaci√≥n relevante para an√°lisis sem√°ntico.",
            metadata={
                'category': category,
                'source': sources[i % len(sources)],
                'duration': 30 + np.random.randint(0, 300),
                'confidence': 0.7 + np.random.random() * 0.3,
                'speaker_count': np.random.randint(1, 5),
                'language': 'es'
            },
            category=category,
            timestamp=time.time() - np.random.randint(0, 86400 * 30),  # √öltimo mes
            audio_file_path=f"test_audio_{i:04d}.wav"
        )

        documents.append(doc)

    print(f"‚úÖ {len(documents)} documentos generados")
    return documents

def generate_test_queries(num_queries: int, embedding_dim: int) -> List[np.ndarray]:
    """Genera consultas de prueba"""
    print(f"üîç Generando {num_queries} consultas de prueba...")

    np.random.seed(123)  # Seed diferente para consultas
    queries = []

    for i in range(num_queries):
        # Generar consulta con estructura similar a los documentos
        query_embedding = np.random.randn(embedding_dim)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        queries.append(query_embedding)

    print(f"‚úÖ {len(queries)} consultas generadas")
    return queries

def test_single_database(db_type: VectorDBType, configurator, test_data: List[VectorDocument],
                        test_queries: List[np.ndarray]) -> Dict[str, Any]:
    """Prueba una base de datos espec√≠fica"""

    # Crear configuraci√≥n espec√≠fica
    config = configurator.get_vector_db_config(db_type)

    # Ajustes espec√≠ficos para pruebas
    if db_type == VectorDBType.FAISS:
        config.faiss_index_type = "flat"  # M√°s simple para pruebas
        config.faiss_gpu = False
        config.faiss_index_path = str(OUTPUT_ROOT / "faiss_test_index.bin")

    elif db_type == VectorDBType.CHROMADB:
        config.chromadb_path = str(OUTPUT_ROOT / "chromadb_test")
        config.chromadb_collection_name = "test_collection"

    # Crear base de datos
    db = create_vector_database(config)

    # M√©tricas de prueba
    metrics = {
        "db_type": db_type.value,
        "config": config.__dict__.copy(),
        "initialization_time": 0.0,
        "insertion_time": 0.0,
        "insertion_rate": 0.0,
        "search_times": [],
        "avg_search_time": 0.0,
        "search_rate": 0.0,
        "memory_usage": 0.0,
        "accuracy_metrics": {},
        "errors": []
    }

    try:
        # 1. Inicializaci√≥n
        print(f"üöÄ Inicializando {db_type.value}...")
        start_time = time.time()
        if not db.initialize():
            raise Exception("Error en inicializaci√≥n")
        metrics["initialization_time"] = time.time() - start_time
        print(f"   ‚è±Ô∏è  Inicializaci√≥n: {metrics['initialization_time']:.3f}s")

        # 2. Inserci√≥n de documentos
        print(f"üì• Insertando {len(test_data)} documentos...")
        start_time = time.time()

        # Insertar en lotes para mejor rendimiento
        batch_size = 100
        for i in range(0, len(test_data), batch_size):
            batch = test_data[i:i+batch_size]
            if not db.add_documents(batch):
                raise Exception(f"Error insertando lote {i//batch_size + 1}")

        insertion_time = time.time() - start_time
        metrics["insertion_time"] = insertion_time
        metrics["insertion_rate"] = len(test_data) / insertion_time
        print(f"   ‚è±Ô∏è  Inserci√≥n: {insertion_time:.3f}s ({metrics['insertion_rate']:.1f} docs/s)")

        # 3. Pruebas de b√∫squeda
        print(f"üîç Realizando {len(test_queries)} b√∫squedas...")
        search_times = []
        search_results = []

        for i, query in enumerate(test_queries):
            start_time = time.time()
            results = db.search(query, k=10)
            search_time = time.time() - start_time

            search_times.append(search_time)
            search_results.append(results)

            if i % 10 == 0:
                print(f"   üîç B√∫squeda {i+1}/{len(test_queries)}: {search_time:.3f}s ({len(results)} resultados)")

        metrics["search_times"] = search_times
        metrics["avg_search_time"] = np.mean(search_times)
        metrics["search_rate"] = 1.0 / metrics["avg_search_time"] if metrics["avg_search_time"] > 0 else 0
        print(f"   ‚è±Ô∏è  B√∫squeda promedio: {metrics['avg_search_time']:.3f}s ({metrics['search_rate']:.1f} b√∫squedas/s)")

        # 4. Evaluaci√≥n de precisi√≥n
        print(f"üìä Evaluando precisi√≥n...")
        accuracy_metrics = evaluate_search_accuracy(test_data, test_queries, search_results)
        metrics["accuracy_metrics"] = accuracy_metrics

        # 5. Estad√≠sticas de la base de datos
        db_stats = db.get_statistics()
        metrics["db_statistics"] = db_stats
        print(f"   üìä Documentos almacenados: {db_stats.get('document_count', 0)}")

        # 6. Limpiar
        if hasattr(db, 'clear'):
            db.clear()

        print(f"‚úÖ Prueba de {db_type.value} completada exitosamente")

    except Exception as e:
        error_msg = f"Error en {db_type.value}: {e}"
        print(f"‚ùå {error_msg}")
        metrics["errors"].append(error_msg)

    return metrics

def evaluate_search_accuracy(test_data: List[VectorDocument], test_queries: List[np.ndarray],
                           search_results: List[List]) -> Dict[str, float]:
    """Eval√∫a la precisi√≥n de los resultados de b√∫squeda"""

    if not search_results or not test_queries:
        return {"error": "No hay resultados para evaluar"}

    # M√©tricas b√°sicas
    total_searches = len(search_results)
    successful_searches = sum(1 for results in search_results if len(results) > 0)
    avg_results_per_search = np.mean([len(results) for results in search_results])

    # Evaluar coherencia de categor√≠as
    category_coherence_scores = []
    for results in search_results:
        if len(results) == 0:
            continue

        categories = [r.document.category for r in results if r.document.category]
        if len(categories) > 1:
            # Calcular coherencia (proporci√≥n de la categor√≠a m√°s com√∫n)
            from collections import Counter
            category_counts = Counter(categories)
            most_common_count = category_counts.most_common(1)[0][1]
            coherence = most_common_count / len(categories)
            category_coherence_scores.append(coherence)

    avg_category_coherence = np.mean(category_coherence_scores) if category_coherence_scores else 0.0

    # Evaluar distribuci√≥n de puntajes de similitud
    similarity_scores = []
    for results in search_results:
        for result in results:
            similarity_scores.append(result.similarity_score)

    return {
        "success_rate": successful_searches / total_searches,
        "avg_results_per_search": avg_results_per_search,
        "avg_category_coherence": avg_category_coherence,
        "similarity_score_mean": np.mean(similarity_scores) if similarity_scores else 0.0,
        "similarity_score_std": np.std(similarity_scores) if similarity_scores else 0.0,
        "total_searches": total_searches,
        "successful_searches": successful_searches
    }

def generate_comparison_report(results: Dict[str, Dict[str, Any]]):
    """Genera reporte de comparaci√≥n entre bases de datos"""
    print(f"\n{'='*80}")
    print("üìä REPORTE DE COMPARACI√ìN DE BASES DE DATOS VECTORIALES")
    print(f"{'='*80}")

    # Crear tabla de comparaci√≥n
    comparison_data = []

    for db_name, metrics in results.items():
        if "error" in metrics:
            comparison_data.append({
                "Database": db_name.upper(),
                "Status": "‚ùå ERROR",
                "Init Time": "N/A",
                "Insert Rate": "N/A",
                "Search Time": "N/A",
                "Success Rate": "N/A",
                "Error": metrics["error"]
            })
            continue

        comparison_data.append({
            "Database": db_name.upper(),
            "Status": "‚úÖ OK",
            "Init Time": f"{metrics.get('initialization_time', 0):.3f}s",
            "Insert Rate": f"{metrics.get('insertion_rate', 0):.1f} docs/s",
            "Search Time": f"{metrics.get('avg_search_time', 0):.3f}s",
            "Search Rate": f"{metrics.get('search_rate', 0):.1f} q/s",
            "Success Rate": f"{metrics.get('accuracy_metrics', {}).get('success_rate', 0)*100:.1f}%",
            "Coherence": f"{metrics.get('accuracy_metrics', {}).get('avg_category_coherence', 0)*100:.1f}%"
        })

    # Mostrar tabla
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        print("\nüìà RESUMEN DE RENDIMIENTO:")
        print(df.to_string(index=False))

    # Detalles espec√≠ficos
    print(f"\nüìã DETALLES POR BASE DE DATOS:")
    for db_name, metrics in results.items():
        print(f"\nüî∏ {db_name.upper()}:")

        if "error" in metrics:
            print(f"   ‚ùå Error: {metrics['error']}")
            continue

        print(f"   ‚ö° Inicializaci√≥n: {metrics.get('initialization_time', 0):.3f}s")
        print(f"   üì• Inserci√≥n: {metrics.get('insertion_rate', 0):.1f} documentos/segundo")
        print(f"   üîç B√∫squeda: {metrics.get('avg_search_time', 0):.3f}s promedio")

        accuracy = metrics.get('accuracy_metrics', {})
        if accuracy:
            print(f"   üìä Precisi√≥n:")
            print(f"      ‚Ä¢ Tasa de √©xito: {accuracy.get('success_rate', 0)*100:.1f}%")
            print(f"      ‚Ä¢ Coherencia de categor√≠as: {accuracy.get('avg_category_coherence', 0)*100:.1f}%")
            print(f"      ‚Ä¢ Resultados promedio: {accuracy.get('avg_results_per_search', 0):.1f}")

    # Recomendaciones
    print(f"\nüí° RECOMENDACIONES:")

    successful_dbs = [db for db, metrics in results.items() if "error" not in metrics]

    if not successful_dbs:
        print("   ‚ö†Ô∏è  Ninguna base de datos funcion√≥ correctamente")
        return

    # Mejor en velocidad de inserci√≥n
    fastest_insert = max(successful_dbs,
                        key=lambda db: results[db].get('insertion_rate', 0))
    print(f"   üöÄ Mejor para inserci√≥n masiva: {fastest_insert.upper()}")

    # Mejor en velocidad de b√∫squeda
    fastest_search = min(successful_dbs,
                        key=lambda db: results[db].get('avg_search_time', float('inf')))
    print(f"   ‚ö° Mejor para b√∫squedas r√°pidas: {fastest_search.upper()}")

    # Mejor precisi√≥n
    most_accurate = max(successful_dbs,
                       key=lambda db: results[db].get('accuracy_metrics', {}).get('success_rate', 0))
    print(f"   üéØ Mejor precisi√≥n: {most_accurate.upper()}")

    print(f"\nüíæ Casos de uso recomendados:")
    print(f"   ‚Ä¢ Desarrollo/Prototipado: MEMORY")
    print(f"   ‚Ä¢ Producci√≥n local: FAISS")
    print(f"   ‚Ä¢ Aplicaciones web: CHROMADB")
    print(f"   ‚Ä¢ Producci√≥n escalable: SUPABASE")

    # Guardar resultados detallados
    output_dir = OUTPUT_ROOT
    output_dir.mkdir(exist_ok=True)

    results_file = output_dir / "vector_db_comparison_results.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nüíæ Resultados detallados guardados en: {results_file}")

def main():
    """Funci√≥n principal"""
    try:
        success = test_vector_databases()
        if success:
            print("\nüèÜ TESTS COMPLETADOS EXITOSAMENTE")
            return 0
        else:
            print("\nüí• TESTS FALLARON")
            return 1
    except KeyboardInterrupt:
        print("\nüõë Tests interrumpidos")
        return 1
    except Exception as e:
        print(f"\nüí• Error inesperado: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
