#!/usr/bin/env python3
"""
Prueba r√°pida del sistema de benchmark de embeddings.
Ejecuta una evaluaci√≥n peque√±a para verificar que todo funciona.
"""

import sys
import logging
from pathlib import Path

CURRENT_FILE = Path(__file__).resolve()
TESTS_ROOT = CURRENT_FILE
while TESTS_ROOT.name != "tests" and TESTS_ROOT.parent != TESTS_ROOT:
    TESTS_ROOT = TESTS_ROOT.parent
if str(TESTS_ROOT) not in sys.path:
    sys.path.insert(0, str(TESTS_ROOT))

from tests.common.path_utils import artifacts_dir

BENCHMARK_ARTIFACTS = artifacts_dir("benchmark_quick")

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_quick_benchmark():
    """Ejecuta una prueba r√°pida del benchmark"""
    print("üß™ Prueba R√°pida del Sistema de Benchmark")
    print("=" * 50)

    try:
        # Importar desde el script principal
        sys.path.insert(0, '.')
        from run_embedding_benchmark import ComprehensiveEmbeddingBenchmark

        # Crear benchmark con directorio de prueba
        benchmark = ComprehensiveEmbeddingBenchmark(str(BENCHMARK_ARTIFACTS))

        print("\nüîç Verificando dependencias...")
        dependencies = benchmark.check_dependencies()

        # Verificar que tenemos al menos un modelo funcional
        available_models = sum([
            dependencies.get("torch", False) or dependencies.get("tensorflow", False),
            dependencies.get("transformers", False)
        ])

        if available_models < 2:
            print("‚ö†Ô∏è  Dependencias insuficientes para prueba completa")
            print("üí° Instalando dependencias con: pip install -r requirements_benchmark.txt")
            return False

        print("\nüìä Generando datos de prueba peque√±os...")
        success = benchmark.generate_test_data(
            num_samples=20,  # Muy peque√±o para prueba r√°pida
            num_queries=5,
            regenerate=True
        )

        if not success:
            print("‚ùå Error generando datos de prueba")
            return False

        print("\nüéØ Ejecutando evaluaci√≥n r√°pida...")

        # Solo evaluar modelos disponibles
        available_models = []
        if dependencies.get("tensorflow", False):
            available_models.append("yamnet")
        if dependencies.get("torch", False) and dependencies.get("transformers", False):
            available_models.append("speechdpr")

        if not available_models:
            print("‚ùå No hay modelos disponibles para evaluar")
            return False

        print(f"ü§ñ Evaluando modelos: {available_models}")

        success = benchmark.run_model_evaluations(available_models)

        if success:
            print("‚úÖ Evaluaci√≥n completada exitosamente")

            # Mostrar resultados b√°sicos
            if benchmark.benchmark.results:
                print("\nüìä Resultados:")
                for model_name, metrics in benchmark.benchmark.results.items():
                    print(f"   {model_name}:")
                    print(f"     - BERTScore F1: {metrics.bert_score_f1:.3f}")
                    print(f"     - Consultas exitosas: {metrics.successful_queries}/{metrics.total_queries}")
                    print(f"     - Tiempo promedio: {metrics.query_time:.3f}s")

            print("\nüìä Generando visualizaciones...")
            benchmark.generate_visualizations()

            print("\nüìã Creando reporte...")
            benchmark.create_summary_report()

            print(f"\n‚úÖ Prueba completada. Resultados en: {benchmark.output_dir}")
            return True
        else:
            print("‚ùå Error en evaluaci√≥n")
            return False

    except Exception as e:
        print(f"‚ùå Error en prueba: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    try:
        success = test_quick_benchmark()
        print("\n" + "=" * 50)
        if success:
            print("üéâ ¬°Prueba exitosa! El sistema de benchmark est√° funcionando correctamente.")
            print("üí° Para ejecutar benchmark completo: python run_embedding_benchmark.py")
        else:
            print("‚ö†Ô∏è  La prueba no se complet√≥ exitosamente. Verifica las dependencias.")

        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\nüõë Prueba interrumpida")
        sys.exit(1)
