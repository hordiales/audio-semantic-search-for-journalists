#!/usr/bin/env python3
"""
Test de componentes individuales del framework sin cargar modelos pesados.
Verifica que cada componente funcione independientemente.
"""

import sys
import os
import logging
from pathlib import Path

CURRENT_FILE = Path(__file__).resolve()
TESTS_ROOT = CURRENT_FILE
while TESTS_ROOT.name != "tests" and TESTS_ROOT.parent != TESTS_ROOT:
    TESTS_ROOT = TESTS_ROOT.parent
if str(TESTS_ROOT) not in sys.path:
    sys.path.insert(0, str(TESTS_ROOT))

from tests.common.path_utils import artifacts_dir, ensure_sys_path, SRC_ROOT

COMPONENTS_ARTIFACTS = artifacts_dir("components")

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_imports():
    """Verifica que todos los imports funcionen correctamente"""
    print("ğŸ“¦ Probando imports de mÃ³dulos...")

    results = {}

    # Test import de generador de datos
    try:
        ensure_sys_path([SRC_ROOT])
        from test_data_generator import SyntheticTestDataGenerator
        results['test_data_generator'] = True
        print("âœ… test_data_generator importado")
    except Exception as e:
        results['test_data_generator'] = False
        print(f"âŒ test_data_generator: {e}")

    # Test import de visualization dashboard
    try:
        from visualization_dashboard import EmbeddingVisualizationDashboard
        results['visualization_dashboard'] = True
        print("âœ… visualization_dashboard importado")
    except Exception as e:
        results['visualization_dashboard'] = False
        print(f"âŒ visualization_dashboard: {e}")

    # Test import de embedding evaluation framework
    try:
        from embedding_evaluation_framework import EmbeddingBenchmark, TestCase, EvaluationMetrics
        results['embedding_evaluation_framework'] = True
        print("âœ… embedding_evaluation_framework importado")
    except Exception as e:
        results['embedding_evaluation_framework'] = False
        print(f"âŒ embedding_evaluation_framework: {e}")

    # Test import de models config
    try:
        from models_config import get_models_config, AudioEmbeddingModel
        results['models_config'] = True
        print("âœ… models_config importado")
    except Exception as e:
        results['models_config'] = False
        print(f"âŒ models_config: {e}")

    # Test import de audio embeddings (sin cargar modelos)
    try:
        from audio_embeddings import AudioEmbeddingGenerator
        results['audio_embeddings'] = True
        print("âœ… audio_embeddings importado")
    except Exception as e:
        results['audio_embeddings'] = False
        print(f"âŒ audio_embeddings: {e}")

    return results

def test_data_generation_only():
    """Prueba solo la generaciÃ³n de datos sin modelos"""
    print("ğŸ”§ Probando generaciÃ³n de datos...")

    try:
        ensure_sys_path([SRC_ROOT])
        from test_data_generator import SyntheticTestDataGenerator

        # Crear generador
        generator = SyntheticTestDataGenerator(str(COMPONENTS_ARTIFACTS))

        # Generar dataset muy pequeÃ±o
        df = generator.generate_test_dataset(num_samples=5)

        # Verificaciones bÃ¡sicas
        assert len(df) == 5
        assert 'text' in df.columns
        assert 'category' in df.columns

        print(f"âœ… Dataset generado: {len(df)} muestras")

        # Generar consultas
        queries = generator.create_ground_truth_queries(df, num_queries=2)
        assert len(queries) == 2

        print(f"âœ… Consultas generadas: {len(queries)}")
        return True

    except Exception as e:
        print(f"âŒ Error en generaciÃ³n de datos: {e}")
        return False

def test_metrics_calculations():
    """Prueba cÃ¡lculos de mÃ©tricas sin modelos"""
    print("ğŸ“Š Probando cÃ¡lculos de mÃ©tricas...")

    success_count = 0
    total_tests = 0

    # Test BERTScore si estÃ¡ disponible
    total_tests += 1
    try:
        from bert_score import score as bert_score

        predictions = ["El gobierno anunciÃ³ nuevas medidas", "Los mercados reaccionaron positivamente"]
        references = ["El estado presentÃ³ polÃ­ticas nuevas", "Los mercados tuvieron reacciÃ³n favorable"]

        P, R, F1 = bert_score(predictions, references, lang="es", verbose=False)

        print(f"âœ… BERTScore: P={P.mean():.3f}, R={R.mean():.3f}, F1={F1.mean():.3f}")
        success_count += 1

    except ImportError:
        print("âš ï¸  BERTScore no disponible")
    except Exception as e:
        print(f"âŒ Error en BERTScore: {e}")

    # Test Sentence Transformers
    total_tests += 1
    try:
        from sentence_transformers import SentenceTransformer
        from sklearn.metrics.pairwise import cosine_similarity

        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        texts = ["polÃ­tica econÃ³mica", "medidas econÃ³micas", "tecnologÃ­a digital"]
        embeddings = model.encode(texts)
        similarities = cosine_similarity(embeddings)

        print(f"âœ… Sentence Transformers: similitud calculada {similarities[0,1]:.3f}")
        success_count += 1

    except ImportError:
        print("âš ï¸  Sentence Transformers no disponible")
    except Exception as e:
        print(f"âŒ Error en Sentence Transformers: {e}")

    # Test mÃ©tricas bÃ¡sicas de recuperaciÃ³n
    total_tests += 1
    try:
        # Simular resultados
        ground_truth = [1, 3, 5]
        top_5_results = [1, 2, 3, 4, 5]

        precision_at_5 = len(set(top_5_results).intersection(set(ground_truth))) / 5
        recall_at_5 = len(set(top_5_results).intersection(set(ground_truth))) / len(ground_truth)

        print(f"âœ… MÃ©tricas de recuperaciÃ³n: P@5={precision_at_5:.3f}, R@5={recall_at_5:.3f}")
        success_count += 1

    except Exception as e:
        print(f"âŒ Error en mÃ©tricas de recuperaciÃ³n: {e}")

    return success_count, total_tests

def test_visualization_creation():
    """Prueba creaciÃ³n de visualizaciones bÃ¡sicas"""
    print("ğŸ“ˆ Probando creaciÃ³n de visualizaciones...")

    try:
        import matplotlib.pyplot as plt
        import numpy as np

        # Datos sintÃ©ticos para prueba
        models = ['Model A', 'Model B', 'Model C']
        scores = [0.75, 0.82, 0.78]

        # Crear grÃ¡fico simple
        fig, ax = plt.subplots(figsize=(8, 5))
        bars = ax.bar(models, scores, color=['skyblue', 'lightgreen', 'coral'])
        ax.set_title('Test Visualization')
        ax.set_ylabel('Score')

        # Guardar
        output_dir = COMPONENTS_ARTIFACTS
        output_dir.mkdir(exist_ok=True)
        plt.savefig(output_dir / "test_viz.png", dpi=150)
        plt.close()

        print("âœ… VisualizaciÃ³n bÃ¡sica creada")
        return True

    except Exception as e:
        print(f"âŒ Error en visualizaciÃ³n: {e}")
        return False

def test_config_loading():
    """Prueba carga de configuraciÃ³n"""
    print("âš™ï¸  Probando carga de configuraciÃ³n...")

    try:
        ensure_sys_path([SRC_ROOT])
        from models_config import get_models_config, AudioEmbeddingModel, ModelsConfigLoader

        # Test enum
        assert AudioEmbeddingModel.YAMNET.value == "yamnet"
        assert AudioEmbeddingModel.SPEECHDPR.value == "speechdpr"
        print("âœ… Enums de modelos funcionan")

        # Test loader
        loader = ModelsConfigLoader()
        config = loader.load_config()

        if config is not None:
            print(f"âœ… ConfiguraciÃ³n cargada: {type(config)}")
        else:
            print("âš ï¸  ConfiguraciÃ³n retornÃ³ None")

        return True

    except Exception as e:
        print(f"âŒ Error en configuraciÃ³n: {e}")
        return False

def test_framework_structure():
    """Verifica la estructura bÃ¡sica del framework"""
    print("ğŸ—ï¸  Probando estructura del framework...")

    try:
        ensure_sys_path([SRC_ROOT])
        from embedding_evaluation_framework import EmbeddingBenchmark, TestCase, EvaluationMetrics

        # Test creaciÃ³n de TestCase
        test_case = TestCase(
            query_text="test query",
            expected_keywords=["test", "keyword"],
            category="test",
            difficulty="easy"
        )

        assert test_case.query_text == "test query"
        print("âœ… TestCase funciona")

        # Test creaciÃ³n de EvaluationMetrics
        metrics = EvaluationMetrics(model_name="test_model")
        assert metrics.model_name == "test_model"
        print("âœ… EvaluationMetrics funciona")

        # Test inicializaciÃ³n de benchmark (sin cargar modelos)
        eval_dir = COMPONENTS_ARTIFACTS / "eval"
        eval_dir.mkdir(parents=True, exist_ok=True)
        benchmark = EmbeddingBenchmark(str(eval_dir))
        assert benchmark.output_dir.exists()
        print("âœ… EmbeddingBenchmark se inicializa")

        return True

    except Exception as e:
        print(f"âŒ Error en estructura del framework: {e}")
        return False

def main():
    """FunciÃ³n principal de test de componentes"""
    print("ğŸ§ª Test de Componentes del Framework")
    print("=" * 60)

    tests = [
        ("Imports de mÃ³dulos", test_imports),
        ("ConfiguraciÃ³n", test_config_loading),
        ("Estructura del framework", test_framework_structure),
        ("GeneraciÃ³n de datos", test_data_generation_only),
        ("CÃ¡lculos de mÃ©tricas", test_metrics_calculations),
        ("CreaciÃ³n de visualizaciones", test_visualization_creation),
    ]

    results = {}
    total_success = 0

    for test_name, test_func in tests:
        print(f"\n{'='*60}")
        print(f"ğŸ” Ejecutando: {test_name}")
        print(f"{'='*60}")

        try:
            if test_name == "CÃ¡lculos de mÃ©tricas":
                success_count, total_tests = test_func()
                success = success_count == total_tests
                print(f"ğŸ“Š MÃ©tricas: {success_count}/{total_tests} exitosas")
            elif test_name == "Imports de mÃ³dulos":
                import_results = test_func()
                success = all(import_results.values())
                successful_imports = sum(import_results.values())
                total_imports = len(import_results)
                print(f"ğŸ“¦ Imports: {successful_imports}/{total_imports} exitosos")
            else:
                success = test_func()

            results[test_name] = success

            if success:
                print(f"âœ… {test_name}: EXITOSO")
                total_success += 1
            else:
                print(f"âŒ {test_name}: FALLÃ“")

        except Exception as e:
            print(f"ğŸ’¥ {test_name}: ERROR INESPERADO - {e}")
            results[test_name] = False

    # Resumen final
    print(f"\n{'='*60}")
    print("ğŸ“Š RESUMEN DE COMPONENTES")
    print(f"{'='*60}")

    for test_name, success in results.items():
        status = "âœ… PASS" if success else "âŒ FAIL"
        print(f"{status} {test_name}")

    total_tests = len(results)
    print(f"\nğŸ“ˆ Resultado: {total_success}/{total_tests} componentes funcionando")

    if total_success == total_tests:
        print("ğŸ‰ Â¡Todos los componentes funcionan correctamente!")
        print("ğŸ’¡ El framework estÃ¡ listo para uso (sin cargar modelos pesados)")
    elif total_success >= total_tests * 0.8:
        print("âœ¨ La mayorÃ­a de componentes funcionan. Sistema es funcional.")
    else:
        print("âš ï¸  MÃºltiples componentes tienen problemas. Revisa las dependencias.")

    return total_success == total_tests

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ›‘ Tests interrumpidos")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Error inesperado: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
